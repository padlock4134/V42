import ollama from "ollama/browser";
import { BaseEmbedding } from "../embeddings/types.js";
import { extractText, streamConverter } from "./utils.js";
const messageAccessor = (part)=>{
    return {
        raw: part,
        delta: part.message.content
    };
};
const completionAccessor = (part)=>{
    return {
        text: part.response,
        raw: part
    };
};
/**
 * This class both implements the LLM and Embedding interfaces.
 */ export class Ollama extends BaseEmbedding {
    hasStreaming = true;
    // https://ollama.ai/library
    model;
    options = {
        num_ctx: 4096,
        top_p: 0.9,
        temperature: 0.7
    };
    constructor(params){
        super();
        this.model = params.model;
        if (params.options) {
            this.options = {
                ...this.options,
                ...params.options
            };
        }
    }
    get metadata() {
        const { temperature, top_p, num_ctx } = this.options;
        return {
            model: this.model,
            temperature: temperature,
            topP: top_p,
            maxTokens: undefined,
            contextWindow: num_ctx,
            tokenizer: undefined
        };
    }
    async chat(params) {
        const { messages, stream } = params;
        const payload = {
            model: this.model,
            messages: messages.map((message)=>({
                    role: message.role,
                    content: extractText(message.content)
                })),
            stream: !!stream,
            options: {
                ...this.options
            }
        };
        if (!stream) {
            const chatResponse = await ollama.chat({
                ...payload,
                stream: false
            });
            return {
                message: {
                    role: "assistant",
                    content: chatResponse.message.content
                },
                raw: chatResponse
            };
        } else {
            const stream = await ollama.chat({
                ...payload,
                stream: true
            });
            return streamConverter(stream, messageAccessor);
        }
    }
    async complete(params) {
        const { prompt, stream } = params;
        const payload = {
            model: this.model,
            prompt: extractText(prompt),
            stream: !!stream,
            options: {
                ...this.options
            }
        };
        if (!stream) {
            const response = await ollama.generate({
                ...payload,
                stream: false
            });
            return {
                text: response.response,
                raw: response
            };
        } else {
            const stream = await ollama.generate({
                ...payload,
                stream: true
            });
            return streamConverter(stream, completionAccessor);
        }
    }
    async getEmbedding(prompt) {
        const payload = {
            model: this.model,
            prompt,
            options: {
                ...this.options
            }
        };
        const response = await ollama.embeddings({
            ...payload
        });
        return response.embedding;
    }
    async getTextEmbedding(text) {
        return this.getEmbedding(text);
    }
    async getQueryEmbedding(query) {
        return this.getEmbedding(query);
    }
    // ollama specific methods, inherited from the ollama library
    static async list() {
        const { models } = await ollama.list();
        return models;
    }
    static async detail(modelName, options) {
        return ollama.show({
            model: modelName,
            ...options
        });
    }
    static async create(modelName, options) {
        return ollama.create({
            model: modelName,
            ...options,
            stream: options ? !!options.stream : false
        });
    }
    static async delete(modelName) {
        return ollama.delete({
            model: modelName
        });
    }
    static async copy(source, destination) {
        return ollama.copy({
            source,
            destination
        });
    }
    static async pull(modelName, options) {
        return ollama.pull({
            model: modelName,
            ...options,
            stream: options ? !!options.stream : false
        });
    }
    static async push(modelName, options) {
        return ollama.push({
            model: modelName,
            ...options,
            stream: options ? !!options.stream : false
        });
    }
}
