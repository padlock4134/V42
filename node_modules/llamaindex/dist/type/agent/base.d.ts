import { type ChatEngine, type ChatEngineParamsNonStreaming, type ChatEngineParamsStreaming } from "../engines/chat/index.js";
import type { ChatMessage, ChatResponse, ChatResponseChunk, LLM, MessageContent } from "../llm/index.js";
import type { BaseToolWithCall, ToolOutput, UUID } from "../types.js";
export declare const MAX_TOOL_CALLS = 10;
export type AgentTaskContext<Model extends LLM, Store extends object = {}, AdditionalMessageOptions extends object = Model extends LLM<object, infer AdditionalMessageOptions> ? AdditionalMessageOptions : never> = {
    readonly stream: boolean;
    readonly toolCallCount: number;
    readonly llm: Model;
    readonly getTools: (input: MessageContent) => BaseToolWithCall[] | Promise<BaseToolWithCall[]>;
    shouldContinue: (taskStep: Readonly<TaskStep<Model, Store, AdditionalMessageOptions>>) => boolean;
    store: {
        toolOutputs: ToolOutput[];
        messages: ChatMessage<AdditionalMessageOptions>[];
    } & Store;
};
export type TaskStep<Model extends LLM, Store extends object = {}, AdditionalMessageOptions extends object = Model extends LLM<object, infer AdditionalMessageOptions> ? AdditionalMessageOptions : never> = {
    id: UUID;
    input: ChatMessage<AdditionalMessageOptions> | null;
    context: AgentTaskContext<Model, Store, AdditionalMessageOptions>;
    prevStep: TaskStep<Model, Store, AdditionalMessageOptions> | null;
    nextSteps: Set<TaskStep<Model, Store, AdditionalMessageOptions>>;
};
export type TaskStepOutput<Model extends LLM, Store extends object = {}, AdditionalMessageOptions extends object = Model extends LLM<object, infer AdditionalMessageOptions> ? AdditionalMessageOptions : never> = {
    taskStep: TaskStep<Model, Store, AdditionalMessageOptions>;
    output: null | ChatResponse<AdditionalMessageOptions> | ReadableStream<ChatResponseChunk<AdditionalMessageOptions>>;
    isLast: false;
} | {
    taskStep: TaskStep<Model, Store, AdditionalMessageOptions>;
    output: ChatResponse<AdditionalMessageOptions> | ReadableStream<ChatResponseChunk<AdditionalMessageOptions>>;
    isLast: true;
};
export type TaskHandler<Model extends LLM, Store extends object = {}, AdditionalMessageOptions extends object = Model extends LLM<object, infer AdditionalMessageOptions> ? AdditionalMessageOptions : never> = (step: TaskStep<Model, Store, AdditionalMessageOptions>) => Promise<TaskStepOutput<Model, Store, AdditionalMessageOptions>>;
/**
 * @internal
 */
export declare function createTaskImpl<Model extends LLM, Store extends object = {}, AdditionalMessageOptions extends object = Model extends LLM<object, infer AdditionalMessageOptions> ? AdditionalMessageOptions : never>(handler: TaskHandler<Model, Store, AdditionalMessageOptions>, context: AgentTaskContext<Model, Store, AdditionalMessageOptions>, _input: ChatMessage<AdditionalMessageOptions>): AsyncGenerator<TaskStepOutput<Model, Store, AdditionalMessageOptions>>;
export type AgentStreamChatResponse<Options extends object> = {
    response: ChatResponseChunk<Options>;
    sources?: ToolOutput[];
};
export type AgentChatResponse<Options extends object> = {
    response: ChatResponse<Options>;
    sources: ToolOutput[];
};
export type AgentRunnerParams<AI extends LLM, Store extends object = {}, AdditionalMessageOptions extends object = AI extends LLM<object, infer AdditionalMessageOptions> ? AdditionalMessageOptions : never> = {
    llm: AI;
    chatHistory: ChatMessage<AdditionalMessageOptions>[];
    systemPrompt: MessageContent | null;
    runner: AgentWorker<AI, Store, AdditionalMessageOptions>;
    tools: BaseToolWithCall[] | ((query: MessageContent) => Promise<BaseToolWithCall[]>);
};
export type AgentParamsBase<AI extends LLM, AdditionalMessageOptions extends object = AI extends LLM<object, infer AdditionalMessageOptions> ? AdditionalMessageOptions : never> = {
    llm?: AI;
    chatHistory?: ChatMessage<AdditionalMessageOptions>[];
    systemPrompt?: MessageContent;
};
/**
 * Worker will schedule tasks and handle the task execution
 */
export declare abstract class AgentWorker<AI extends LLM, Store extends object = {}, AdditionalMessageOptions extends object = AI extends LLM<object, infer AdditionalMessageOptions> ? AdditionalMessageOptions : never> {
    #private;
    abstract taskHandler: TaskHandler<AI, Store, AdditionalMessageOptions>;
    createTask(query: string, context: AgentTaskContext<AI, Store, AdditionalMessageOptions>): ReadableStream<TaskStepOutput<AI, Store, AdditionalMessageOptions>>;
    [Symbol.toStringTag]: string;
}
/**
 * Runner will manage the task execution and provide a high-level API for the user
 */
export declare abstract class AgentRunner<AI extends LLM, Store extends object = {}, AdditionalMessageOptions extends object = AI extends LLM<object, infer AdditionalMessageOptions> ? AdditionalMessageOptions : never> implements ChatEngine<AgentChatResponse<AdditionalMessageOptions>, ReadableStream<AgentStreamChatResponse<AdditionalMessageOptions>>> {
    #private;
    abstract createStore(): Store;
    static defaultCreateStore(): object;
    protected constructor(params: AgentRunnerParams<AI, Store, AdditionalMessageOptions>);
    get llm(): AI;
    get chatHistory(): ChatMessage<AdditionalMessageOptions>[];
    reset(): void;
    getTools(query: MessageContent): Promise<BaseToolWithCall[]> | BaseToolWithCall[];
    static shouldContinue<AI extends LLM, Store extends object = {}, AdditionalMessageOptions extends object = AI extends LLM<object, infer AdditionalMessageOptions> ? AdditionalMessageOptions : never>(task: Readonly<TaskStep<AI, Store, AdditionalMessageOptions>>): boolean;
    createTask(message: MessageContent, stream?: boolean): Promise<ReadableStream<TaskStepOutput<AI, Store, AdditionalMessageOptions>>>;
    chat(params: ChatEngineParamsNonStreaming): Promise<AgentChatResponse<AdditionalMessageOptions>>;
    chat(params: ChatEngineParamsStreaming): Promise<ReadableStream<AgentStreamChatResponse<AdditionalMessageOptions>>>;
}
