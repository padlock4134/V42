import { GoogleGenerativeAI } from "@google/generative-ai";
import { ToolCallLLM } from "./base.js";
import type { ChatResponse, ChatResponseChunk, CompletionResponse, LLMChatParamsNonStreaming, LLMChatParamsStreaming, LLMCompletionParamsNonStreaming, LLMCompletionParamsStreaming, LLMMetadata, ToolCallLLMMessageOptions } from "./types.js";
type GeminiSessionOptions = {
    apiKey?: string;
};
export declare enum GEMINI_MODEL {
    GEMINI_PRO = "gemini-pro",
    GEMINI_PRO_VISION = "gemini-pro-vision",
    EMBEDDING_001 = "embedding-001",
    AQA = "aqa",
    GEMINI_PRO_LATEST = "gemini-1.5-pro-latest"
}
export interface GeminiModelInfo {
    contextWindow: number;
}
export declare const GEMINI_MODEL_INFO_MAP: Record<GEMINI_MODEL, GeminiModelInfo>;
declare const DEFAULT_GEMINI_PARAMS: {
    model: GEMINI_MODEL;
    temperature: number;
    topP: number;
    maxTokens: undefined;
};
export type GeminiConfig = Partial<typeof DEFAULT_GEMINI_PARAMS> & {
    session?: GeminiSession;
};
export type GeminiAdditionalChatOptions = {};
export type GeminiChatParamsStreaming = LLMChatParamsStreaming<GeminiAdditionalChatOptions, ToolCallLLMMessageOptions>;
export type GeminiChatStreamResponse = AsyncIterable<ChatResponseChunk<ToolCallLLMMessageOptions>>;
export type GeminiChatParamsNonStreaming = LLMChatParamsNonStreaming<GeminiAdditionalChatOptions, ToolCallLLMMessageOptions>;
export type GeminiChatNonStreamResponse = ChatResponse<ToolCallLLMMessageOptions>;
/**
 * Gemini Session to manage the connection to the Gemini API
 */
export declare class GeminiSession {
    gemini: GoogleGenerativeAI;
    constructor(options: GeminiSessionOptions);
}
/**
 * Gemini Session Store to manage the current Gemini sessions
 */
export declare class GeminiSessionStore {
    static sessions: Array<{
        session: GeminiSession;
        options: GeminiSessionOptions;
    }>;
    private static sessionMatched;
    static get(options?: GeminiSessionOptions): GeminiSession;
}
/**
 * ToolCallLLM for Gemini
 */
export declare class Gemini extends ToolCallLLM<GeminiAdditionalChatOptions> {
    model: GEMINI_MODEL;
    temperature: number;
    topP: number;
    maxTokens?: number;
    session: GeminiSession;
    constructor(init?: GeminiConfig);
    get supportToolCall(): boolean;
    get metadata(): LLMMetadata;
    private prepareChat;
    protected nonStreamChat(params: GeminiChatParamsNonStreaming): Promise<GeminiChatNonStreamResponse>;
    protected streamChat(params: GeminiChatParamsStreaming): GeminiChatStreamResponse;
    chat(params: GeminiChatParamsStreaming): Promise<GeminiChatStreamResponse>;
    chat(params: GeminiChatParamsNonStreaming): Promise<GeminiChatNonStreamResponse>;
    complete(params: LLMCompletionParamsStreaming): Promise<AsyncIterable<CompletionResponse>>;
    complete(params: LLMCompletionParamsNonStreaming): Promise<CompletionResponse>;
}
export {};
